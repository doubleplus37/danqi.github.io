<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>My first blog</title>
    <url>/posts/11206.html</url>
    <content><![CDATA[<p>为了组队记录自己的学习过程注册的Blog,希望今后能够坚持下来！</p>
<p>网站是根据这一篇博客建立的：<a href="https://echoshanyushi.github.io/2020/02/07/34289/">here</a></p>
]]></content>
      <tags>
        <tag>单琦</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习初学课程</title>
    <url>/posts/4661ff3d.html</url>
    <content><![CDATA[<h2 id="1-导论"><a href="#1-导论" class="headerlink" title="1.导论"></a>1.导论</h2><ul>
<li>ML用于数据挖掘、推荐系统、以及学习人类大脑如何运作。</li>
</ul>
<blockquote>
<p>什么是机器学习？</p>
</blockquote>
<p>Arthur Samuel(1959)：gives computers the ability to learn without being explicitly programmed.</p>
<p>Tom Mitchell(1998): 对于任务T，通过完成T的经验E，来提高对于任务T的完成率。</p>
<a id="more"></a>
<h2 id="2-监督以及无监督学习"><a href="#2-监督以及无监督学习" class="headerlink" title="2.监督以及无监督学习"></a>2.监督以及无监督学习</h2><ul>
<li>监督学习：给出一系列<strong>真实的</strong>输入，并告诉了其对应的输出，从而对其建模，比如回归问题、分类问题</li>
<li>无监督学习：没有给出正确的答案，只是给出一组数据，让机器自己找出结构。比如分类算法</li>
</ul>
<p>无监督学习就像给你39年的降水数据，自动帮你识别出特征(类似于EOF)，比如南方多雨、北方少雨；而对于监督学习就是你除了给机器39年降水数据外，你还要告诉它这一年究竟是属于哪一种，比如三类雨型的某种，从而建立模型，之后就利用模型预测。</p>
<h2 id="3-模型描述"><a href="#3-模型描述" class="headerlink" title="3.模型描述"></a>3.模型描述</h2><p><strong>h -&gt; hypothesis</strong></p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%90%B4%E6%81%A9%E8%BE%BE1/image-20200817181232325.png" alt="image-20200817181232325"></p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%90%B4%E6%81%A9%E8%BE%BE1/image-20200817185203513.png" alt="image-20200817185203513"></p>
<h3 id="3-1-代价函数-cost-function"><a href="#3-1-代价函数-cost-function" class="headerlink" title="3.1 代价函数(cost function)"></a>3.1 代价函数(cost function)</h3><p>目的是为了得到最优的参数，比如线性回归就是最小二乘函数<strong>最小</strong></p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%90%B4%E6%81%A9%E8%BE%BE1/image-20200817185241613.png" alt="image-20200817185241613"></p>
<p>代价函数的参数为(θ0，θ1)，老师没有直接用导数的方法来求得最优化值，而是画出了代价函数 <strong>J</strong> 随θ的变化图像，二维参数使用等高线。</p>
<h3 id="3-2-梯度下降"><a href="#3-2-梯度下降" class="headerlink" title="3.2 梯度下降"></a>3.2 梯度下降</h3><p>最优化任意函数 <strong>J(θ,…)</strong></p>
<blockquote>
<p>步骤</p>
</blockquote>
<ul>
<li>给定参数初值</li>
<li>从初值开始，找到一个下降最快的方向，前进一小步</li>
<li>不断前进</li>
</ul>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%90%B4%E6%81%A9%E8%BE%BE1/image-20200817190256057.png" alt="image-20200817190256057"></p>
<p><img src="/images/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%90%B4%E6%81%A9%E8%BE%BE1/image-20200817191426758.png" alt="image-20200817191426758"></p>
<p>同时更新θ0和θ1，右边的步骤是错误的。</p>
]]></content>
      <categories>
        <category>机器学习课程</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>吴恩达</tag>
        <tag>机器学习入门</tag>
      </tags>
  </entry>
  <entry>
    <title>datawhale机器学习(一)-逻辑回归</title>
    <url>/posts/cf91a0de.html</url>
    <content><![CDATA[<h2 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1.摘要"></a>1.摘要</h2><p>logit回归是一种二元或多元回归，属于广义线性回归。其与一般的线性回归的区别在于：logit回归值为离散，而线性回归值为连续。其通常用于分类问题，从机器学习的角度来说，其属于监督学习，即在训练时同时给出预测结果，这是其与聚类分析的差别。logit回归使用的是非线性模型，因为其引入了<code>sigmod函数</code>:$y=\frac{1}{1+e^{-z}}$，其中$z = \theta^T·x$，$\theta$为要训练的参数。</p>
<a id="more"></a>
<h2 id="2-内容"><a href="#2-内容" class="headerlink" title="2.内容"></a>2.内容</h2><blockquote>
<p> <strong>思考以及学习的内容围绕助教的5个问题</strong></p>
</blockquote>
<ol>
<li>什么是逻辑回归，逻辑回归的推导，损失函数的推导 </li>
<li>逻辑回归与SVM的异同 </li>
<li>逻辑回归与线性回归的不同 </li>
<li>为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好 </li>
<li>LR为什么用Sigmoid函数，这个函数有什么优缺点，为什么不用其他函数</li>
</ol>
<h3 id="2-1-什么是逻辑回归？"><a href="#2-1-什么是逻辑回归？" class="headerlink" title="2.1 什么是逻辑回归？"></a>2.1 什么是逻辑回归？</h3><p><strong>1.概念</strong></p>
<p>所谓逻辑回归，本质上是一个分类问题，其预测值为一组离散值，在模型中一般不直接预测结果的值，<strong>而是预测每个离散结果出现的概率</strong>，并且选取出现概率最大的值作为预测值。在二元逻辑回归就常表现为选取概率大于0.5的一方，即将$z=0$作为决策边界。</p>
<p><strong>2.逻辑回归推导</strong></p>
<p>…不明觉厉</p>
<p><strong>3.损失函数的推导</strong></p>
<p>所谓损失函数，就是衡量模式对于训练数据的预测值与真实值之间的差异程度，是训练模型的重要一环，一般来说，模型的训练都会朝着损失函数减小的方向进行。</p>
<blockquote>
<p>这里借鉴一下其他大佬的总结：</p>
<ul>
<li><p>选择代价函数时，最好挑选对参数$\theta$可微的函数（全微分存在，偏导数一定存在）</p>
</li>
<li><p>对于每种算法来说，代价函数不是唯一的；</p>
</li>
<li><p>代价函数是参数 $\theta$的函数；</p>
</li>
<li><p>总的代价函数 $J(\theta)$ 可以用来评价模型的好坏，代价函数越小说明模型和参数越符合训练样本 $(x,y)$ ；</p>
</li>
<li><p>$J(\theta)$ 是一个标量；</p>
<p>链接:<a href="https://zhuanlan.zhihu.com/p/28408516">https://zhuanlan.zhihu.com/p/28408516</a></p>
</li>
</ul>
</blockquote>
<p><strong>对于二元逻辑回归的损失函数具体推导如下</strong></p>
<p>假设y只能取0或1</p>
<script type="math/tex; mode=display">
P(y=0\mid x;\theta)=h_\theta(x)\\
P(y=1\mid x;\theta)=1-h_\theta(x)</script><p>将这两个公式统一起来</p>
<script type="math/tex; mode=display">
P(y\mid x;\theta) = h_\theta(x)^{1-y}+(1-h_\theta(x))^y</script><p>上面这个公式是针对某一个y值建立的，即针对一个样本；但是模型的建立是通过训练一组样本，即总体最优化。</p>
<p><strong>建立似然函数</strong></p>
<script type="math/tex; mode=display">
P(y\mid x;\theta) = \prod h_\theta(x_i)^{1-y_i}+(1-h_\theta(x_i))^y_i</script><p>这里采用似然函数建立模型，表示以样本x为预测因子，并且以计算出来的$\theta$作为分布的参数，发生样本y(y为一组)发生的概率最大，其与损失函数结果类似，但思想略有不同。当然还有通过信息熵方面的理论含义来直接建立损失函数。</p>
<p><strong>对数似然</strong></p>
<script type="math/tex; mode=display">
lnP(y\mid x;\theta) = \sum (1-y_i)h_\theta(x_i)+y_i(1-h_\theta(x_i))</script><p>用样本训练$\theta$使得$lnP$最大，如果令$C=-\frac{1}{M}lnP$，则C的含义类似于损失函数，要训练使得C最小。</p>
<h3 id="2-2-SVM与逻辑回归的不同"><a href="#2-2-SVM与逻辑回归的不同" class="headerlink" title="2.2 SVM与逻辑回归的不同"></a>2.2 SVM与逻辑回归的不同</h3><p>还没有看SVM的部分，先占坑，过几天补。。。</p>
<h3 id="2-3-逻辑回归与线性回归的差别"><a href="#2-3-逻辑回归与线性回归的差别" class="headerlink" title="2.3 逻辑回归与线性回归的差别"></a>2.3 逻辑回归与线性回归的差别</h3><p>逻辑回归与线性回归同属广义线性回归模型，但主要的差别如下：</p>
<ol>
<li>逻辑回归本质是一个分类问题，而线性回归则是回归问题。这一点也体现在预测量上，逻辑回归中预测量为离散值，而线性回归则为连续值。</li>
<li>逻辑回归表面上<strong>因变量</strong>是离散值，但其实际模型的预测量是概率，是一个连续值，只不过通过概率的大小来给出离散值的结果，因此离散和连续的特点并不直接表现在模型的关键部分。</li>
<li>线性回归假设因变量为高斯分布，而2分类问题假设因变量为伯努利分布(概率论知识，网上看的，不明觉厉)</li>
<li>采用的损失函数不同，线性回归的损失函数是以最小二乘法为基础，即选用平方损失函数。</li>
</ol>
<h3 id="2-4-为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好-？"><a href="#2-4-为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好-？" class="headerlink" title="2.4 为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好 ？"></a>2.4 为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好 ？</h3><blockquote>
<p> 为什么LR需要归一化或者对数？</p>
</blockquote>
<p>不同特征的量级不同，而量级差异会使得采用迭代方法计算参数$\theta$收敛的很慢或者不能收敛。</p>
<p><img src="../images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%AF%BE(logit%E5%9B%9E%E5%BD%92" alt="img">/20160517192219846)</p>
<p>如上图，会使得用于梯度下降的等高线变得更“圆”，而不是椭圆。</p>
<p>图片链接：<a href="https://blog.csdn.net/weixin_38111819/article/details/79729444">https://blog.csdn.net/weixin_38111819/article/details/79729444</a></p>
<blockquote>
<p>为什么LR把特征离散化后效果会更好？</p>
</blockquote>
<p>这个问题好复杂…从来没有想过这个问题，看了一下网上的答案，主要的原因包括：可以使得运算速度变快(从迭代变快以及稀疏向量内积的方向考虑)；增加鲁棒性；增加模型的非线性属性(可以将单变量离散为N个，每个变量就有了单独的权重)；降低了过拟合风险。</p>
<p>很多不太能理解，先占坑<a href="https://www.zhihu.com/question/31989952">https://www.zhihu.com/question/31989952</a></p>
<h3 id="2-5-LR为什么选用sigmod函数"><a href="#2-5-LR为什么选用sigmod函数" class="headerlink" title="2.5 LR为什么选用sigmod函数"></a>2.5 LR为什么选用sigmod函数</h3><p>其实二分类问题最简单的判断函数为：</p>
<script type="math/tex; mode=display">
f(x)=\left\{
\begin{aligned}
1(x>0) \\
0.5(x=0) \\
0(x<0)
\end{aligned}
\right.</script><p>但是跃迁函数在$x=0$不可微会带来很多问题，而<code>sigmod</code>函数则不存在这个问题，其在<code>x=0</code>即$f(x)=\frac12$处是敏感的，即具有跃迁函数的性质，在$z=\frac12$处x微小的差异就可以使得z有较大的差异，从而利于分类。</p>
<p>而对于缺点：主要是这个函数的输出不是均值为0的，因此假设输入均为正数（或负数），那么对w的导数总是正数（或负数），使得收敛缓慢。此外,<code>sigmod</code>函数作为指数函数计算较慢。</p>
<p>参考链接：<a href="https://zhuanlan.zhihu.com/p/71882757">https://zhuanlan.zhihu.com/p/71882757</a></p>
<p><a href="https://blog.csdn.net/NOT_GUY/article/details/78749509">https://blog.csdn.net/NOT_GUY/article/details/78749509</a></p>
<blockquote>
<p>关于为什么选用sigmod函数，网络上的解释太概率论，等看懂了再回来。</p>
</blockquote>
]]></content>
      <categories>
        <category>datawhale机器学习入门</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习入门</tag>
        <tag>datawhale</tag>
      </tags>
  </entry>
  <entry>
    <title>datawhale机器学习(二)-决策树</title>
    <url>/posts/f862962d.html</url>
    <content><![CDATA[<h1 id="1-决策树理论"><a href="#1-决策树理论" class="headerlink" title="1.决策树理论"></a>1.决策树理论</h1><h2 id="1-1-概要"><a href="#1-1-概要" class="headerlink" title="1.1 概要"></a>1.1 概要</h2><p>决策树属于分类模型，且属于监督学习。总的模型架构类似于编程中不断嵌套的<code>if-else</code>语句，通过训练数据建立模型，在预测时将预测数据的各个特征代入模型，经过不断的判断(分支)，可以到达一个不同特征满足一定条件的区域，在模型中被称为<strong>叶节点</strong>。在这个区域中，所有的输入数据都将对应同一个输出，即被预测为同一个类别。</p>
<a id="more"></a>
<p><strong>举个例子</strong></p>
<p><img src="../images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91/index.png" alt="index"></p>
<p><img src="../images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91/index-1598104710164.png" alt="index"></p>
<blockquote>
<p>图片来自：<a href="https://zhuanlan.zhihu.com/p/26703300">https://zhuanlan.zhihu.com/p/26703300</a> (西瓜书)</p>
</blockquote>
<h2 id="1-2-模型的建立"><a href="#1-2-模型的建立" class="headerlink" title="1.2 模型的建立"></a>1.2 模型的建立</h2><p>对于决策树模型的建立，最关键的步骤在于：</p>
<ol>
<li>树怎么长，也就是树怎么分支，关于这个问题有很多种算法，比如<code>ID3算法</code>，<code>C4.5算法</code>，<code>CART算法</code>等，不同的算法可能会导致分支的标准不同。</li>
<li>树长到什么时候结束，树如果长的太浅，就会导致欠拟合，即最后的叶节点中含有较大的不同类型占比，信息较混乱，会导致预测错误率较高；如果长得太深，则可能会导致过拟合问题。</li>
</ol>
<h3 id="1-2-1-树怎么长"><a href="#1-2-1-树怎么长" class="headerlink" title="1.2.1 树怎么长"></a><strong>1.2.1 树怎么长</strong></h3><p>决策树算法的关键就在于其如何分裂，因此需要建立指标来衡量分裂的好坏。根据实际情况，我们希望树分支后的各个节点里的数据类型较为<strong>单一</strong>，也就是较”纯”，不同的算法其度量方法也不一样。</p>
<p><strong>ID3算法</strong></p>
<p>将<code>信息熵</code>的概念引入到模型中来计算信息的纯度，通过计算<code>信息增益</code>来确定模型。</p>
<p><strong>信息熵的定义为:</strong><script type="math/tex">Ent(D)=-\sum_{k=1}^{n}P_k log_2P_k</script></p>
<p>D代表当前的样本集合,n代表当前的样本可以分为n类。</p>
<p><strong>条件熵的定义为：</strong></p>
<p>将当前的样本集合D按照某一个<strong>特征</strong>分类，假该特征有m个离散属性，那么假设按照该特征分类，条件熵为：</p>
<script type="math/tex; mode=display">\sum_{i=1}^{m} \frac {Num(D_i)}{Num(D)}Ent(D_i)</script><p>$Num(D_i)$表示不同离散属性对应得样本数量，$Num(D)$则代表总数量，相除用以表示权重</p>
<p><strong>信息增益</strong></p>
<p>用属性a对样本D进行分类</p>
<script type="math/tex; mode=display">Gain(D,a)=Ent(D)-\sum_{i=1}^{m} \frac {Num(D_i)}{Num(D)}Ent(D_i)</script><p>其实引入<code>条件熵</code>这个概念就足够了,因为对于一个已知的样本集D，其<code>信息熵</code>是确定的，信息熵代表了信息的混乱程度，信息熵越大，代表样本中的类别越不纯。而条件熵其实就代表的是按照这个标准分叉之后的样本纯度，信息增益为它们之差，代表的是样本混乱程度的减少情况，因此，信息增益越大，代表该分类越好。</p>
<blockquote>
<p>下面是一个例子，来自<a href="https://zhuanlan.zhihu.com/p/26760551">https://zhuanlan.zhihu.com/p/26760551</a></p>
</blockquote>
<p> <img src="../images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91/index-1598104738266.png" alt="index"></p>
<p><strong>ID3算法的缺点：</strong> 其对于含有较多分类的属性来说有偏好性，网上大家都举的是将编号作为一个特征进行分类，那么其得到的信息增益是最大的，这是因为每一个编号都只对应一个输出，因为$logP=0$，其总的条件熵也为0。</p>
<p>而从本质上来说，这应该是一个过拟合问题，选用类别较多的特征进行分类，其分出来的各个子样本虽然纯度高，但是数量少，会使得模型不具有泛化能力。</p>
<p><strong>其他算法</strong></p>
<p>为了改进ID3算法的缺点，提出了C4.5算法，其提出了信息增益率的概念，即给信息增益乘了一个系数，这个系数对于类别多的特征来说会很小，从而平衡上面的问题，但是其由于在分母上，又会存在偏好类别少的样本的特点。</p>
<p>上面的ID3和C4.5算法都是基于信息熵作为衡量指标进行分类，而CART算法则提出了<code>基尼系数</code>的概念，CART算法是一个二叉树模型。</p>
<p>基尼系数用来表示在一个样本集合中，同时抽取两个样本，这两个样本所属的类别不同的概率。用基尼系数来衡量样本的纯度，基尼系数越小，则样本越纯。</p>
<blockquote>
<p>关于三种算法\<br><a href="https://www.cnblogs.com/mantch/p/11145186.html">https://www.cnblogs.com/mantch/p/11145186.html</a> \<br><a href="https://www.cnblogs.com/pinard/p/6050306.html">https://www.cnblogs.com/pinard/p/6050306.html</a> \</p>
<p>算法的代码实现\<br><a href="https://blog.csdn.net/v_july_v/article/details/7577684">https://blog.csdn.net/v_july_v/article/details/7577684</a></p>
</blockquote>
<h2 id="2-决策树算法的优缺点及技巧"><a href="#2-决策树算法的优缺点及技巧" class="headerlink" title="2.决策树算法的优缺点及技巧"></a>2.决策树算法的优缺点及技巧</h2><p><strong>优点：</strong></p>
<ol>
<li>简单直观，生成的决策树很直观.</li>
</ol>
<ol>
<li>基本不需要预处理，不需要提前归一化，处理缺失值。</li>
</ol>
<ol>
<li>使用决策树预测的代价是O(log2m)</li>
</ol>
<ol>
<li>既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。</li>
</ol>
<ol>
<li>可以处理多维度输出的分类问题。</li>
</ol>
<ol>
<li>相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释</li>
</ol>
<ol>
<li>可以交叉验证的剪枝来选择模型，从而提高泛化能力。</li>
</ol>
<ol>
<li>对于异常点的容错能力好，鲁棒性高。</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。</li>
</ol>
<ol>
<li>决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。</li>
</ol>
<ol>
<li>寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。</li>
</ol>
<ol>
<li>有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</li>
</ol>
<ol>
<li>如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</li>
</ol>
<p><strong>关于过拟合的问题</strong></p>
<p>即树生长到什么程度，避免过拟合的一个主要方法就是<strong>剪枝</strong></p>
<p>剪枝又包括<code>先剪枝</code>和<code>后剪枝</code></p>
<p><strong>先剪枝</strong></p>
<p><code>先剪枝</code>通过限制决策树的高度和叶子结点处样本的数目来阻止决策树的生长</p>
<ul>
<li>定义一个高度，当决策树达到该高度时就可以停止决策树的生长，这是一种最为简单的方法；</li>
</ul>
<ul>
<li>达到某个结点的实例具有相同的特征向量，即使这些实例不属于同一类，也可以停止决策树的生长。这种方法对于处理数据中的数据冲突问题非常有效；</li>
</ul>
<ul>
<li>定义一个阈值，当达到某个结点的实例个数小于该阈值时就可以停止决策树的生长；</li>
</ul>
<ul>
<li>定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值的大小来决定是否停止决策树的生长。</li>
</ul>
<p><strong>后剪枝</strong></p>
<p>后剪枝在实际操作中更常用</p>
<p>后剪枝首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。</p>
<p>后剪枝的剪枝过程是删除一些子树，然后用其叶子节点代替，这个叶子节点所标识的类别通过大多数原则(majority class criterion)确定。所谓大多数原则，是指剪枝过程中, 将一些子树删除而用叶节点代替,这个叶节点所标识的类别用这棵子树中大多数训练样本所属的类别来标识,所标识的类称为majority class .相比于先剪枝，这种方法更常用，正是因为在先剪枝方法中精确地估计何时停止树增长很困难。</p>
]]></content>
      <categories>
        <category>datawhale机器学习入门</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>机器学习入门</tag>
        <tag>datawhale</tag>
      </tags>
  </entry>
  <entry>
    <title>文献阅读(2020/9/14)</title>
    <url>/posts/17b6b017.html</url>
    <content><![CDATA[<h2 id="WHAT-IS-THE-POLAR-VORTEX-AND-HOW-DOES-IT-INFLUENCE-WEATHER"><a href="#WHAT-IS-THE-POLAR-VORTEX-AND-HOW-DOES-IT-INFLUENCE-WEATHER" class="headerlink" title="WHAT IS THE POLAR VORTEX AND HOW DOES IT INFLUENCE WEATHER?"></a>WHAT IS THE POLAR VORTEX AND HOW DOES IT INFLUENCE WEATHER?</h2><p><img src="../images/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB9-14/image-20200914204714247.png" alt="image-20200914204714247" style="zoom:50%;" /></p>
<a id="more"></a>
<blockquote>
<p>Waugh, Darryn W.; Sobel, Adam H.; Polvani, Lorenzo M. (2017): What Is the Polar Vortex and How Does It Influence Weather? In Bulletin of the American Meteorological Society 98 (1), pp. 37–44. DOI: 10.1175/BAMS-D-15-00212.1.</p>
</blockquote>
<p><img src="../images/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB9-14/image-20200914200810281.png" alt="image-20200914200810281"></p>
<p><strong><center>平流层和对流层极涡是不同的系统，在垂直方向上有明显的分界线</center></strong></p>
<ul>
<li><p>作者认为很多文献没有分清到底是对流层极涡还是平流层极涡，这容易产生混淆</p>
</li>
<li><p>媒体通常报导极涡是极端的或者不寻常的，这是不对的，因为极涡一直都存在，是地球系统的一员，一般冷事件都是由于其移动造成的</p>
<p><img src="../images/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB9-14/image-20200914202557576.png" alt="image-20200914202557576"></p>
</li>
</ul>
<center>月际及以上尺度来说，对流层极涡一般有两个中心，一个位于东北西伯利亚，一个位于加拿大东部</center>

<ul>
<li>作者认为虽然平流层和对流层极涡都对地面冷事件有影响，但是对流层极涡影响更大(或者说更直接)，这里我认为作者的意思是对流层极涡和平流层极涡是两个不同的东西，如果把这两个当作一个东西，那么就会夸大平流层极涡的直接作用，事实上他只能通过平-对相互作用来影响。</li>
<li>作者指出虽然冷空气爆发和平流层弱极涡存在统计上的联系，但肯定不是一一对应的，毕竟冷空气爆发毕竟是对流层的事件。</li>
<li>作者认为冷空气的爆发的最关键原因可能不是极涡整体的变化造成的，极涡是一个半球尺度的系统；而正确的解释应该是”极涡边缘在一些地方移动导致的”</li>
<li></li>
</ul>
]]></content>
      <categories>
        <category>文献阅读</category>
      </categories>
      <tags>
        <tag>极涡</tag>
        <tag>文献阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>文献阅读(9-15)</title>
    <url>/posts/ea6fd24c.html</url>
    <content><![CDATA[<h2 id="通过个例阐述AO与MJO对东亚寒潮的影响"><a href="#通过个例阐述AO与MJO对东亚寒潮的影响" class="headerlink" title="通过个例阐述AO与MJO对东亚寒潮的影响"></a><center>通过个例阐述AO与MJO对东亚寒潮的影响</center></h2><p><img src="../images/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-9-15/image-20200915205922760.png" alt="image-20200915205922760"></p>
<a id="more"></a>
<blockquote>
<p>Park, T.‐W., C.‐H. Ho, S. Yang, and J.‐H. Jeong (2010), Influences of Arctic Oscillation and Madden‐Julian Oscillation on cold surges and heavy snowfalls over Korea: A case study for the winter of 2009–2010, J. Geophys. Res., 115, D23122, doi:10.1029/2010JD014794.</p>
</blockquote>
<p><strong>个人感想</strong>：</p>
<ul>
<li><p>西伯利亚高压是东亚寒潮的关键，AO正负位相相对于寒潮可能是间接的系统，不能代表寒潮发生的强弱(文中西伯利亚的阻塞型和波列型值得关注)</p>
</li>
<li><p>MJO影响寒潮的机制还需要看别的文章，但本文讨论了MJO可能是2009年产生雪的重要原因值得借鉴</p>
</li>
</ul>
<p>2009年韩国发生了4次寒潮</p>
<p><img src="../images/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-9-15/image-20200915203732317.png" alt="image-20200915203732317"></p>
<h3 id="将寒潮对应的环流形势分为阻塞型和波列型"><a href="#将寒潮对应的环流形势分为阻塞型和波列型" class="headerlink" title="将寒潮对应的环流形势分为阻塞型和波列型"></a>将寒潮对应的环流形势分为阻塞型和波列型</h3><p><img src="../images/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-9-15/image-20200915202727351.png" width="80%"></p>
<p><strong>填色为850hpa异常，等值线为300Hpa异常</strong></p>
<p>CS1、CS3、CS4是阻塞型，经向梯度比较明显，CS2是波列型，文中说有随高度西倾的现象，但貌似不是很明显</p>
<p>CS2冷空气维持的时间很短，为2天，其他阻塞型维持时间都在8天以上。</p>
<p><img src="../images/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-9-15/image-20200915203637955.png" alt="image-20200915203637955"></p>
<p>作者还指出西伯利亚是关键系统，CS2尽管处于AO负位相，但西伯利亚高压增强不明显导致寒潮时间短，而CS1尽管是AO正位相，但西伯利亚高压有增强，使得寒潮也较强。</p>
<p>作者通过比较不同AO位相的西伯利亚高压得出AO对于寒潮的影响，正AO（CS1）下，西伯利亚高压源于欧亚大陆，向东南发展，而<strong>负AO下，西伯利亚高压来自于极地，类似于阻塞型</strong>:这里有文献引用，后期可以看。</p>
<p>作者还对比了相同AO位相，但是不同MJO位相事件西伯利亚高压之间的差异，差异不明显，得出可能不会直接影响西伯利亚的结论。</p>
<h3 id="MJO的影响"><a href="#MJO的影响" class="headerlink" title="MJO的影响"></a>MJO的影响</h3><p>有文献指出MJO2-3位相时，可能发生寒潮的概率比6-7位相大很多</p>
<p>本文通过用CS4-CS3得到，MJO2-3位相时，500hpa位势高度负距平区域更南，以及大气环流在东亚中纬度的斜压性更强，与前任类似。(不太有说服力，只是一次事件相减)</p>
<p><strong>对于降雪过程的解释</strong></p>
<p>MJO2-3位相可以在中纬度激发波列，从而在韩国上方阐述局地的上升场，有利于降雪。</p>
<p>MJO激发中纬度波列的机制本文采用的哈德莱环流的观点：</p>
<p><img src="../images/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-9-15/image-20200915205050147.png" alt="image-20200915205050147"></p>
<p>对流加强使得哈德莱环流也加强，从而激发出波列如下</p>
<p><img src="../images/%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB-9-15/image-20200915205146686.png" alt="image-20200915205146686"></p>
<p><strong><center>阴影代表rossby波源，90°E附近</center></strong></p>
<p>从而在韩国上方激发出上升场，有利于降雪。</p>
]]></content>
      <categories>
        <category>文献阅读</category>
      </categories>
      <tags>
        <tag>文献阅读</tag>
        <tag>寒潮</tag>
        <tag>AO</tag>
      </tags>
  </entry>
  <entry>
    <title>ENSO对于SSW以及阻塞的调制(文献阅读)</title>
    <url>/posts/caebd006.html</url>
    <content><![CDATA[<p>摘要：这篇文章主要探讨了ENSO对<strong>作为SSW前兆的阻塞</strong>发生是否有调制，比如ENSO下主要是大西洋欧洲区域发生阻塞，太平洋区域阻塞很少；而Lanina则太平洋阻塞、西伯利亚阻塞发生频率增加。同时也探讨了ENSO对不同类型的SSW前兆的阻塞是否有调制。此外，在不同位置的阻塞上传的行星波类型(1、2波)不同，并用”线性叠加效应“解释了原因，<strong>此外相同位置阻塞上传的行星波在不同ENSO位相也不相同，但没有解释原因</strong>。此外，还发现<strong>SSW按1、2波分类与按偏移分裂分类不完全一致</strong>的主要原因在于：ENSO下，即使是分裂型，也主要是1波起作用。除了对作为SSW的前兆阻塞有影响外，作者还讨论了ENSO是否会影响所有阻塞，发现其与前兆阻塞影响类似，所以可能是ENSO通过直接调制阻塞，从而影响SSW的发生。</p>
<a id="more"></a>
<p><strong>本质上是一篇比较水的文章，在前人不同类型SSW前兆阻塞分布的基础上加上了ENSO的影响进行合成分析，但是可能存在样本数偏少的问题。但是得到了一些有趣的结果：比如在ENSO位相下没有1波型SSW发生，当然这里的1、2波判定标准也可能存在主观问题。另外，文章中核心是ENSO位相通过调制阻塞生成的位置以及持续时间来影响上传的行星波，从而影响不同类型SSW的发生。但是文章中也发现在同一区域的阻塞，在不同ENSO位相下上传的行星波种类也有差异，这一点的原因还值得探讨</strong>。</p>
<p><img src="../images/ENSO%E5%AF%B9%E4%BA%8ESSW%E4%BB%A5%E5%8F%8A%E9%98%BB%E5%A1%9E%E7%9A%84%E8%B0%83%E5%88%B6-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20200920142721110.png" alt="image-20200920142721110"></p>
<blockquote>
<p> Barriopedro D ,  Calvo N . On the Relationship between ENSO,  Stratospheric Sudden Warmings, and Blocking[J]. Journal of Climate,  2014, 27(12):4704-4720.</p>
</blockquote>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul>
<li>ENSO下，大西洋欧洲阻塞倾向于作为SSW前兆，而LAnina则是太平洋阻塞。并且在ENSO不同位相，SSW前兆阻塞之间的差异大于偏移分裂SSW前兆阻塞的差异，而与1、2波类型差异相当。这可能说明导致偏移、分裂SSW发生的阻塞前兆差异不是特别大，即相同的阻塞信号有可能产生偏移型也有可能分裂型。</li>
<li>ENSO相比Lanina，大西洋区域的阻塞上传的行星1波要大得多，而在Lanina下，太平洋阻塞导致的2波增强要比其带俩的1波减弱强，总体表现为正值。</li>
</ul>
<p><img src="../images/ENSO%E5%AF%B9%E4%BA%8ESSW%E4%BB%A5%E5%8F%8A%E9%98%BB%E5%A1%9E%E7%9A%84%E8%B0%83%E5%88%B6-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20200920150407739.png" alt="image-20200920150407739"></p>
<ul>
<li>1、2波SSW并不与偏移、分裂一一对应，主要体现在ENSO位相下分裂型SSW也是1波造成的。</li>
</ul>
<p><img src="../images/ENSO%E5%AF%B9%E4%BA%8ESSW%E4%BB%A5%E5%8F%8A%E9%98%BB%E5%A1%9E%E7%9A%84%E8%B0%83%E5%88%B6-%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/image-20200920150535155.png" alt="image-20200920150535155"></p>
<ul>
<li>ENSO下，阻塞的持续时间与作为SSW前兆的阻塞分布相似，可能代表，ENSO通过影响冬季阻塞的持续时间，从而调制SSW的发生(感觉不是太可信，因为ENSO下大西洋阻塞持续时间更长，但是大西洋阻塞又对应着很大概率偏移型SSW的发生，但在ENSO下，偏移型和分裂型SSW没有明显差异)</li>
</ul>
]]></content>
      <categories>
        <category>文献阅读</category>
      </categories>
      <tags>
        <tag>文献阅读</tag>
        <tag>阻塞</tag>
        <tag>SSW</tag>
      </tags>
  </entry>
</search>
