<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.0.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"shanqi1998.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1,avatar:"/images/head.png",url:null,rounded:!1,rotated:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="1.决策树理论1.1 概要决策树属于分类模型，且属于监督学习。总的模型架构类似于编程中不断嵌套的if-else语句，通过训练数据建立模型，在预测时将预测数据的各个特征代入模型，经过不断的判断(分支)，可以到达一个不同特征满足一定条件的区域，在模型中被称为叶节点。在这个区域中，所有的输入数据都将对应同一个输出，即被预测为同一个类别。"><meta property="og:type" content="article"><meta property="og:title" content="datawhale机器学习(二)-决策树"><meta property="og:url" content="http://shanqi1998.github.io/posts/f862962d.html"><meta property="og:site_name" content="37&#39;s casual notes"><meta property="og:description" content="1.决策树理论1.1 概要决策树属于分类模型，且属于监督学习。总的模型架构类似于编程中不断嵌套的if-else语句，通过训练数据建立模型，在预测时将预测数据的各个特征代入模型，经过不断的判断(分支)，可以到达一个不同特征满足一定条件的区域，在模型中被称为叶节点。在这个区域中，所有的输入数据都将对应同一个输出，即被预测为同一个类别。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://shanqi1998.github.io/images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91/index.png"><meta property="og:image" content="http://shanqi1998.github.io/images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91/index-1598104710164.png"><meta property="og:image" content="http://shanqi1998.github.io/images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91/index-1598104738266.png"><meta property="article:published_time" content="2020-08-22T09:52:00.000Z"><meta property="article:modified_time" content="2020-08-22T14:00:59.026Z"><meta property="article:author" content="37"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="机器学习入门"><meta property="article:tag" content="datawhale"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://shanqi1998.github.io/images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91/index.png"><link rel="canonical" href="http://shanqi1998.github.io/posts/f862962d.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>datawhale机器学习(二)-决策树 | 37's casual notes</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">37's casual notes</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">why we learn?</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="heart fa-fw"></i>关于</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://shanqi1998.github.io/posts/f862962d.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="37"><meta itemprop="description" content="学海无涯只能刻舟求剑"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="37's casual notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">datawhale机器学习(二)-决策树</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-08-22 17:52:00 / 修改时间：22:00:59" itemprop="dateCreated datePublished" datetime="2020-08-22T17:52:00+08:00">2020-08-22</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">datawhale机器学习入门</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="1-决策树理论"><a href="#1-决策树理论" class="headerlink" title="1.决策树理论"></a>1.决策树理论</h1><h2 id="1-1-概要"><a href="#1-1-概要" class="headerlink" title="1.1 概要"></a>1.1 概要</h2><p>决策树属于分类模型，且属于监督学习。总的模型架构类似于编程中不断嵌套的<code>if-else</code>语句，通过训练数据建立模型，在预测时将预测数据的各个特征代入模型，经过不断的判断(分支)，可以到达一个不同特征满足一定条件的区域，在模型中被称为<strong>叶节点</strong>。在这个区域中，所有的输入数据都将对应同一个输出，即被预测为同一个类别。</p><a id="more"></a><p><strong>举个例子</strong></p><p><img src="../images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91/index.png" alt="index"></p><p><img src="../images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91/index-1598104710164.png" alt="index"></p><blockquote><p>图片来自：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26703300">https://zhuanlan.zhihu.com/p/26703300</a> (西瓜书)</p></blockquote><h2 id="1-2-模型的建立"><a href="#1-2-模型的建立" class="headerlink" title="1.2 模型的建立"></a>1.2 模型的建立</h2><p>对于决策树模型的建立，最关键的步骤在于：</p><ol><li>树怎么长，也就是树怎么分支，关于这个问题有很多种算法，比如<code>ID3算法</code>，<code>C4.5算法</code>，<code>CART算法</code>等，不同的算法可能会导致分支的标准不同。</li><li>树长到什么时候结束，树如果长的太浅，就会导致欠拟合，即最后的叶节点中含有较大的不同类型占比，信息较混乱，会导致预测错误率较高；如果长得太深，则可能会导致过拟合问题。</li></ol><h3 id="1-2-1-树怎么长"><a href="#1-2-1-树怎么长" class="headerlink" title="1.2.1 树怎么长"></a><strong>1.2.1 树怎么长</strong></h3><p>决策树算法的关键就在于其如何分裂，因此需要建立指标来衡量分裂的好坏。根据实际情况，我们希望树分支后的各个节点里的数据类型较为<strong>单一</strong>，也就是较”纯”，不同的算法其度量方法也不一样。</p><p><strong>ID3算法</strong></p><p>将<code>信息熵</code>的概念引入到模型中来计算信息的纯度，通过计算<code>信息增益</code>来确定模型。</p><p><strong>信息熵的定义为:</strong><script type="math/tex">Ent(D)=-\sum_{k=1}^{n}P_k log_2P_k</script></p><p>D代表当前的样本集合,n代表当前的样本可以分为n类。</p><p><strong>条件熵的定义为：</strong></p><p>将当前的样本集合D按照某一个<strong>特征</strong>分类，假该特征有m个离散属性，那么假设按照该特征分类，条件熵为：</p><script type="math/tex;mode=display">\sum_{i=1}^{m} \frac {Num(D_i)}{Num(D)}Ent(D_i)</script><p>$Num(D_i)$表示不同离散属性对应得样本数量，$Num(D)$则代表总数量，相除用以表示权重</p><p><strong>信息增益</strong></p><p>用属性a对样本D进行分类</p><script type="math/tex;mode=display">Gain(D,a)=Ent(D)-\sum_{i=1}^{m} \frac {Num(D_i)}{Num(D)}Ent(D_i)</script><p>其实引入<code>条件熵</code>这个概念就足够了,因为对于一个已知的样本集D，其<code>信息熵</code>是确定的，信息熵代表了信息的混乱程度，信息熵越大，代表样本中的类别越不纯。而条件熵其实就代表的是按照这个标准分叉之后的样本纯度，信息增益为它们之差，代表的是样本混乱程度的减少情况，因此，信息增益越大，代表该分类越好。</p><blockquote><p>下面是一个例子，来自<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/26760551">https://zhuanlan.zhihu.com/p/26760551</a></p></blockquote><p><img src="../images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BA%8C-%E5%86%B3%E7%AD%96%E6%A0%91/index-1598104738266.png" alt="index"></p><p><strong>ID3算法的缺点：</strong> 其对于含有较多分类的属性来说有偏好性，网上大家都举的是将编号作为一个特征进行分类，那么其得到的信息增益是最大的，这是因为每一个编号都只对应一个输出，因为$logP=0$，其总的条件熵也为0。</p><p>而从本质上来说，这应该是一个过拟合问题，选用类别较多的特征进行分类，其分出来的各个子样本虽然纯度高，但是数量少，会使得模型不具有泛化能力。</p><p><strong>其他算法</strong></p><p>为了改进ID3算法的缺点，提出了C4.5算法，其提出了信息增益率的概念，即给信息增益乘了一个系数，这个系数对于类别多的特征来说会很小，从而平衡上面的问题，但是其由于在分母上，又会存在偏好类别少的样本的特点。</p><p>上面的ID3和C4.5算法都是基于信息熵作为衡量指标进行分类，而CART算法则提出了<code>基尼系数</code>的概念，CART算法是一个二叉树模型。</p><p>基尼系数用来表示在一个样本集合中，同时抽取两个样本，这两个样本所属的类别不同的概率。用基尼系数来衡量样本的纯度，基尼系数越小，则样本越纯。</p><blockquote><p>关于三种算法\<br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/mantch/p/11145186.html">https://www.cnblogs.com/mantch/p/11145186.html</a> \<br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6050306.html">https://www.cnblogs.com/pinard/p/6050306.html</a> \</p><p>算法的代码实现\<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/v_july_v/article/details/7577684">https://blog.csdn.net/v_july_v/article/details/7577684</a></p></blockquote><h2 id="2-决策树算法的优缺点及技巧"><a href="#2-决策树算法的优缺点及技巧" class="headerlink" title="2.决策树算法的优缺点及技巧"></a>2.决策树算法的优缺点及技巧</h2><p><strong>优点：</strong></p><ol><li>简单直观，生成的决策树很直观.</li></ol><ol><li>基本不需要预处理，不需要提前归一化，处理缺失值。</li></ol><ol><li>使用决策树预测的代价是O(log2m)</li></ol><ol><li>既可以处理离散值也可以处理连续值。很多算法只是专注于离散值或者连续值。</li></ol><ol><li>可以处理多维度输出的分类问题。</li></ol><ol><li>相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到很好的解释</li></ol><ol><li>可以交叉验证的剪枝来选择模型，从而提高泛化能力。</li></ol><ol><li>对于异常点的容错能力好，鲁棒性高。</li></ol><p><strong>缺点：</strong></p><ol><li>决策树算法非常容易过拟合，导致泛化能力不强。可以通过设置节点最少样本数量和限制决策树深度来改进。</li></ol><ol><li>决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。</li></ol><ol><li>寻找最优的决策树是一个NP难的问题，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。</li></ol><ol><li>有些比较复杂的关系，决策树很难学习，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。</li></ol><ol><li>如果某些特征的样本比例过大，生成决策树容易偏向于这些特征。这个可以通过调节样本权重来改善。</li></ol><p><strong>关于过拟合的问题</strong></p><p>即树生长到什么程度，避免过拟合的一个主要方法就是<strong>剪枝</strong></p><p>剪枝又包括<code>先剪枝</code>和<code>后剪枝</code></p><p><strong>先剪枝</strong></p><p><code>先剪枝</code>通过限制决策树的高度和叶子结点处样本的数目来阻止决策树的生长</p><ul><li>定义一个高度，当决策树达到该高度时就可以停止决策树的生长，这是一种最为简单的方法；</li></ul><ul><li>达到某个结点的实例具有相同的特征向量，即使这些实例不属于同一类，也可以停止决策树的生长。这种方法对于处理数据中的数据冲突问题非常有效；</li></ul><ul><li>定义一个阈值，当达到某个结点的实例个数小于该阈值时就可以停止决策树的生长；</li></ul><ul><li>定义一个阈值，通过计算每次扩张对系统性能的增益，并比较增益值与该阈值的大小来决定是否停止决策树的生长。</li></ul><p><strong>后剪枝</strong></p><p>后剪枝在实际操作中更常用</p><p>后剪枝首先构造完整的决策树，允许树过度拟合训练数据，然后对那些置信度不够的结点子树用叶子结点来代替，该叶子的类标号用该结点子树中最频繁的类标记。</p><p>后剪枝的剪枝过程是删除一些子树，然后用其叶子节点代替，这个叶子节点所标识的类别通过大多数原则(majority class criterion)确定。所谓大多数原则，是指剪枝过程中, 将一些子树删除而用叶节点代替,这个叶节点所标识的类别用这棵子树中大多数训练样本所属的类别来标识,所标识的类称为majority class .相比于先剪枝，这种方法更常用，正是因为在先剪枝方法中精确地估计何时停止树增长很困难。</p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" rel="tag"># 机器学习入门</a> <a href="/tags/datawhale/" rel="tag"># datawhale</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/cf91a0de.html" rel="prev" title="datawhale机器学习(一)-逻辑回归"><i class="fa fa-chevron-left"></i> datawhale机器学习(一)-逻辑回归</a></div><div class="post-nav-item"></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E5%86%B3%E7%AD%96%E6%A0%91%E7%90%86%E8%AE%BA"><span class="nav-number">1.</span> <span class="nav-text">1.决策树理论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E6%A6%82%E8%A6%81"><span class="nav-number">1.1.</span> <span class="nav-text">1.1 概要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%BB%BA%E7%AB%8B"><span class="nav-number">1.2.</span> <span class="nav-text">1.2 模型的建立</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-1-%E6%A0%91%E6%80%8E%E4%B9%88%E9%95%BF"><span class="nav-number">1.2.1.</span> <span class="nav-text">1.2.1 树怎么长</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E5%8F%8A%E6%8A%80%E5%B7%A7"><span class="nav-number">1.3.</span> <span class="nav-text">2.决策树算法的优缺点及技巧</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">37</p><div class="site-description" itemprop="description">学海无涯只能刻舟求剑</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">4</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">2</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">5</span> <span class="site-state-item-name">标签</span></a></div></nav></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2020/8/15 – <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">37</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动</div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script></body></html>