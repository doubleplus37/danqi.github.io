<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.0.2"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"shanqi1998.github.io",root:"/",scheme:"Gemini",version:"7.8.0",exturl:!1,sidebar:{position:"left",display:"post",padding:18,offset:12,onmobile:!1,avatar:"/images/head.jpg",url:null,rounded:!1,rotated:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="1.摘要logit回归是一种二元或多元回归，属于广义线性回归。其与一般的线性回归的区别在于：logit回归值为离散，而线性回归值为连续。其通常用于分类问题，从机器学习的角度来说，其属于监督学习，即在训练时同时给出预测结果，这是其与聚类分析的差别。logit回归使用的是非线性模型，因为其引入了sigmod函数:$y&#x3D;\frac{1}{1+e^{-z}}$，其中$z &#x3D; \theta^T·x$，$\t"><meta property="og:type" content="article"><meta property="og:title" content="datawhale机器学习第一课(logit回归)"><meta property="og:url" content="http://shanqi1998.github.io/posts/cf91a0de.html"><meta property="og:site_name" content="37&#39;s casual notes"><meta property="og:description" content="1.摘要logit回归是一种二元或多元回归，属于广义线性回归。其与一般的线性回归的区别在于：logit回归值为离散，而线性回归值为连续。其通常用于分类问题，从机器学习的角度来说，其属于监督学习，即在训练时同时给出预测结果，这是其与聚类分析的差别。logit回归使用的是非线性模型，因为其引入了sigmod函数:$y&#x3D;\frac{1}{1+e^{-z}}$，其中$z &#x3D; \theta^T·x$，$\t"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://shanqi1998.github.io/images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%AF%BE(logit%E5%9B%9E%E5%BD%92"><meta property="article:published_time" content="2020-08-20T03:48:03.000Z"><meta property="article:modified_time" content="2020-08-20T12:32:37.325Z"><meta property="article:author" content="37"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="datawhale"><meta property="article:tag" content="机器学习入门"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="http://shanqi1998.github.io/images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%AF%BE(logit%E5%9B%9E%E5%BD%92"><link rel="canonical" href="http://shanqi1998.github.io/posts/cf91a0de.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"en"}</script><title>datawhale机器学习第一课(logit回归) | 37's casual notes</title><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">37's casual notes</h1><span class="logo-line-after"><i></i></span></a><p class="site-subtitle" itemprop="description">why we learn?</p></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="home fa-fw"></i>Home</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="heart fa-fw"></i>About</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="http://shanqi1998.github.io/posts/cf91a0de.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="37"><meta itemprop="description" content="学海无涯只能刻舟求剑"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="37's casual notes"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">datawhale机器学习第一课(logit回归)</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2020-08-20 11:48:03 / Modified: 20:32:37" itemprop="dateCreated datePublished" datetime="2020-08-20T11:48:03+08:00">2020-08-20</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" itemprop="url" rel="index"><span itemprop="name">datawhale机器学习入门</span></a></span></span></div></header><div class="post-body" itemprop="articleBody"><h2 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1.摘要"></a>1.摘要</h2><p>logit回归是一种二元或多元回归，属于广义线性回归。其与一般的线性回归的区别在于：logit回归值为离散，而线性回归值为连续。其通常用于分类问题，从机器学习的角度来说，其属于监督学习，即在训练时同时给出预测结果，这是其与聚类分析的差别。logit回归使用的是非线性模型，因为其引入了<code>sigmod函数</code>:$y=\frac{1}{1+e^{-z}}$，其中$z = \theta^T·x$，$\theta$为要训练的参数。</p><a id="more"></a><h2 id="2-内容"><a href="#2-内容" class="headerlink" title="2.内容"></a>2.内容</h2><blockquote><p><strong>思考以及学习的内容围绕助教的5个问题</strong></p></blockquote><ol><li>什么是逻辑回归，逻辑回归的推导，损失函数的推导</li><li>逻辑回归与SVM的异同</li><li>逻辑回归与线性回归的不同</li><li>为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好</li><li>LR为什么用Sigmoid函数，这个函数有什么优缺点，为什么不用其他函数</li></ol><h3 id="2-1-什么是逻辑回归？"><a href="#2-1-什么是逻辑回归？" class="headerlink" title="2.1 什么是逻辑回归？"></a>2.1 什么是逻辑回归？</h3><p><strong>1.概念</strong></p><p>所谓逻辑回归，本质上是一个分类问题，其预测值为一组离散值，在模型中一般不直接预测结果的值，<strong>而是预测每个离散结果出现的概率</strong>，并且选取出现概率最大的值作为预测值。在二元逻辑回归就常表现为选取概率大于0.5的一方，即将$z=0$作为决策边界。</p><p><strong>2.逻辑回归推导</strong></p><p>…不明觉厉</p><p><strong>3.损失函数的推导</strong></p><p>所谓损失函数，就是衡量模式对于训练数据的预测值与真实值之间的差异程度，是训练模型的重要一环，一般来说，模型的训练都会朝着损失函数减小的方向进行。</p><blockquote><p>这里借鉴一下其他大佬的总结：</p><ul><li><p>选择代价函数时，最好挑选对参数$\theta$可微的函数（全微分存在，偏导数一定存在）</p></li><li><p>对于每种算法来说，代价函数不是唯一的；</p></li><li><p>代价函数是参数 $\theta$的函数；</p></li><li><p>总的代价函数 $J(\theta)$ 可以用来评价模型的好坏，代价函数越小说明模型和参数越符合训练样本 $(x,y)$ ；</p></li><li><p>$J(\theta)$ 是一个标量；</p><p>链接:<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/28408516">https://zhuanlan.zhihu.com/p/28408516</a></p></li></ul></blockquote><p><strong>对于二元逻辑回归的损失函数具体推导如下</strong></p><p>假设y只能取0或1</p><script type="math/tex;mode=display">P(y=0\mid x;\theta)=h_\theta(x)\\
P(y=1\mid x;\theta)=1-h_\theta(x)</script><p>将这两个公式统一起来</p><script type="math/tex;mode=display">P(y\mid x;\theta) = h_\theta(x)^{1-y}+(1-h_\theta(x))^y</script><p>上面这个公式是针对某一个y值建立的，即针对一个样本；但是模型的建立是通过训练一组样本，即总体最优化。</p><p><strong>建立似然函数</strong></p><script type="math/tex;mode=display">P(y\mid x;\theta) = \prod h_\theta(x_i)^{1-y_i}+(1-h_\theta(x_i))^y_i</script><p>这里采用似然函数建立模型，表示以样本x为预测因子，并且以计算出来的$\theta$作为分布的参数，发生样本y(y为一组)发生的概率最大，其与损失函数结果类似，但思想略有不同。当然还有通过信息熵方面的理论含义来直接建立损失函数。</p><p><strong>对数似然</strong></p><script type="math/tex;mode=display">lnP(y\mid x;\theta) = \sum (1-y_i)h_\theta(x_i)+y_i(1-h_\theta(x_i))</script><p>用样本训练$\theta$使得$lnP$最大，如果令$C=-\frac{1}{M}lnP$，则C的含义类似于损失函数，要训练使得C最小。</p><h3 id="2-2-SVM与逻辑回归的不同"><a href="#2-2-SVM与逻辑回归的不同" class="headerlink" title="2.2 SVM与逻辑回归的不同"></a>2.2 SVM与逻辑回归的不同</h3><p>还没有看SVM的部分，先占坑，过几天补。。。</p><h3 id="2-3-逻辑回归与线性回归的差别"><a href="#2-3-逻辑回归与线性回归的差别" class="headerlink" title="2.3 逻辑回归与线性回归的差别"></a>2.3 逻辑回归与线性回归的差别</h3><p>逻辑回归与线性回归同属广义线性回归模型，但主要的差别如下：</p><ol><li>逻辑回归本质是一个分类问题，而线性回归则是回归问题。这一点也体现在预测量上，逻辑回归中预测量为离散值，而线性回归则为连续值。</li><li>逻辑回归表面上<strong>因变量</strong>是离散值，但其实际模型的预测量是概率，是一个连续值，只不过通过概率的大小来给出离散值的结果，因此离散和连续的特点并不直接表现在模型的关键部分。</li><li>线性回归假设因变量为高斯分布，而2分类问题假设因变量为伯努利分布(概率论知识，网上看的，不明觉厉)</li><li>采用的损失函数不同，线性回归的损失函数是以最小二乘法为基础，即选用平方损失函数。</li></ol><h3 id="2-4-为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好-？"><a href="#2-4-为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好-？" class="headerlink" title="2.4 为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好 ？"></a>2.4 为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好 ？</h3><blockquote><p>为什么LR需要归一化或者对数？</p></blockquote><p>不同特征的量级不同，而量级差异会使得采用迭代方法计算参数$\theta$收敛的很慢或者不能收敛。</p><p><img src="../images/datawhale%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%AC%E4%B8%80%E8%AF%BE(logit%E5%9B%9E%E5%BD%92" alt="img">/20160517192219846)</p><p>如上图，会使得用于梯度下降的等高线变得更“圆”，而不是椭圆。</p><p>图片链接：<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_38111819/article/details/79729444">https://blog.csdn.net/weixin_38111819/article/details/79729444</a></p><blockquote><p>为什么LR把特征离散化后效果会更好？</p></blockquote><p>这个问题好复杂…从来没有想过这个问题，看了一下网上的答案，主要的原因包括：可以使得运算速度变快(从迭代变快以及稀疏向量内积的方向考虑)；增加鲁棒性；增加模型的非线性属性(可以将单变量离散为N个，每个变量就有了单独的权重)；降低了过拟合风险。</p><p>很多不太能理解，先占坑<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/31989952">https://www.zhihu.com/question/31989952</a></p><h3 id="2-5-LR为什么选用sigmod函数"><a href="#2-5-LR为什么选用sigmod函数" class="headerlink" title="2.5 LR为什么选用sigmod函数"></a>2.5 LR为什么选用sigmod函数</h3><p>其实二分类问题最简单的判断函数为：</p><script type="math/tex;mode=display">f(x)=\left\{
\begin{aligned}
1(x>0) \\
0.5(x=0) \\
0(x<0)
\end{aligned}
\right.</script><p>但是跃迁函数在$x=0$不可微会带来很多问题，而<code>sigmod</code>函数则不存在这个问题，其在<code>x=0</code>即$f(x)=\frac12$处是敏感的，即具有跃迁函数的性质，在$z=\frac12$处x微小的差异就可以使得z有较大的差异，从而利于分类。</p><p>而对于缺点：主要是这个函数的输出不是均值为0的，因此假设输入均为正数（或负数），那么对w的导数总是正数（或负数），使得收敛缓慢。此外,<code>sigmod</code>函数作为指数函数计算较慢。</p><p>参考链接：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/71882757">https://zhuanlan.zhihu.com/p/71882757</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/NOT_GUY/article/details/78749509">https://blog.csdn.net/NOT_GUY/article/details/78749509</a></p><blockquote><p>关于为什么选用sigmod函数，网络上的解释太概率论，等看懂了再回来。</p></blockquote></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a> <a href="/tags/datawhale/" rel="tag"># datawhale</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8/" rel="tag"># 机器学习入门</a></div><div class="post-nav"><div class="post-nav-item"><a href="/posts/4661ff3d.html" rel="prev" title="机器学习初学课程"><i class="fa fa-chevron-left"></i> 机器学习初学课程</a></div><div class="post-nav-item"></div></div></footer></article></div><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">1.摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E5%86%85%E5%AE%B9"><span class="nav-number">2.</span> <span class="nav-text">2.内容</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E4%BB%80%E4%B9%88%E6%98%AF%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%9F"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 什么是逻辑回归？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-SVM%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E4%B8%8D%E5%90%8C"><span class="nav-number">2.2.</span> <span class="nav-text">2.2 SVM与逻辑回归的不同</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%B7%AE%E5%88%AB"><span class="nav-number">2.3.</span> <span class="nav-text">2.3 逻辑回归与线性回归的差别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-%E4%B8%BA%E4%BB%80%E4%B9%88LR%E9%9C%80%E8%A6%81%E5%BD%92%E4%B8%80%E5%8C%96%E6%88%96%E8%80%85%E5%8F%96%E5%AF%B9%E6%95%B0%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88LR%E6%8A%8A%E7%89%B9%E5%BE%81%E7%A6%BB%E6%95%A3%E5%8C%96%E5%90%8E%E6%95%88%E6%9E%9C%E6%9B%B4%E5%A5%BD-%EF%BC%9F"><span class="nav-number">2.4.</span> <span class="nav-text">2.4 为什么LR需要归一化或者取对数，为什么LR把特征离散化后效果更好 ？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-LR%E4%B8%BA%E4%BB%80%E4%B9%88%E9%80%89%E7%94%A8sigmod%E5%87%BD%E6%95%B0"><span class="nav-number">2.5.</span> <span class="nav-text">2.5 LR为什么选用sigmod函数</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">37</p><div class="site-description" itemprop="description">学海无涯只能刻舟求剑</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">3</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">2</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">5</span> <span class="site-state-item-name">tags</span></a></div></nav></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2020</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">37</span></div><div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="user"></i> </span><span class="site-uv" title="Total Visitors"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="eye"></i> </span><span class="site-pv" title="Total Views"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script>if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }</script></body></html>